{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jayanth Kadalipura Shivalingaiah 50290276\n",
    "# Anirudh Manjunath 50289832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(infile,outfile):\n",
    "    with open(infile, 'r',errors='ignore') as f:\n",
    "        cleaned = []\n",
    "        for line in f:\n",
    "            new_line = re.sub('[^ a-zA-Z]', '', line)\n",
    "            #print(new_line)\n",
    "            cleaned.append(new_line)\n",
    "            cleaned.append(\"\\n\")\n",
    "    with open(outfile,'w') as o:\n",
    "        print(outfile)\n",
    "        for i in range(0,len(cleaned)):\n",
    "            try:\n",
    "                #print(\"c\")\n",
    "                o.write(str(cleaned[i]))\n",
    "            except Exception:\n",
    "                print(\"bad write\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanall(flag):\n",
    "    infiles = [\"google\",\"amazon\",\"microsoft\",\"facebook\",\"Uber\"]\n",
    "    outfiles = [\"CleanTwG\",\"CleanTwAZ\",\"CleanTwMS\",\"CleanTwFB\",\"CleanTwU\"]\n",
    "    outfilesny = [\"CleanNYG\",\"CleanNYAZ\",\"CleanNYMS\",\"CleanNYFB\",\"CleanNYU\"]\n",
    "    outcc = [\"Clean-cc-G\",\"Clean-cc-AZ\",\"Clean-cc-MS\",\"Clean-cc-FB\",\"Clean-cc-U\"]\n",
    "    for i in range(0,len(infiles)):\n",
    "        if flag==\"Twitter\":\n",
    "            in_file = \"D:\\\\sem2\\\\DIC\\\\\"+infiles[i]+\".txt\"\n",
    "            out_file = \"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\\"+outfiles[i]+\".txt\"\n",
    "        if flag==\"NYtimes\":\n",
    "            in_file=\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\RawData\\\\\"+infiles[i]+\".txt\"\n",
    "            out_file=\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\\"+outfilesny[i]+\".txt\"\n",
    "        if flag==\"cc\":\n",
    "            in_file = \"D:\\sem2\\dic\\Lab2\\cc\\RawData\\\\\"+infiles[i]+\".txt\"\n",
    "            out_file=\"D:\\sem2\\dic\\Lab2\\cc\\RawData\\\\\"+outcc[i]+\".txt\"\n",
    "        clean(in_file,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\sem2\\dic\\Lab2\\cc\\RawData\\Clean-cc-G.txt\n",
      "D:\\sem2\\dic\\Lab2\\cc\\RawData\\Clean-cc-AZ.txt\n",
      "D:\\sem2\\dic\\Lab2\\cc\\RawData\\Clean-cc-MS.txt\n",
      "D:\\sem2\\dic\\Lab2\\cc\\RawData\\Clean-cc-FB.txt\n",
      "D:\\sem2\\dic\\Lab2\\cc\\RawData\\Clean-cc-U.txt\n",
      "D:\\sem2\\dic\\Lab2\\cc\\RawData\\Clean-cc-all.txt\n"
     ]
    }
   ],
   "source": [
    "flag_var=[\"Twitter\",\"NYtimes\",\"cc\"]\n",
    "#flag_var=[\"cc\"]\n",
    "for i in flag_var:\n",
    "    cleanall(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwG.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwAZ.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwMS.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwFB.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwU.txt\"] \n",
    "with open('D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanAllTw.txt', 'a+') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                x =(' '.join([word for word in line.split() if word not in stop_words or word !=\"a\" or word !=\"U\"]))\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_ny= [\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYG.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYAZ.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYMS.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYFB.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYU.txt\"]\n",
    "with open('D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleanAllNY.txt', 'w+') as outfile:\n",
    "    for fname in filenames_ny:\n",
    "        xx= []\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\RawData\\\\Clean-cc-G.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\RawData\\\\Clean-cc-AZ.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\RawData\\\\Clean-cc-MS.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\RawData\\\\Clean-cc-FB.txt\",\n",
    "             \"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\RawData\\\\Clean-cc-U.txt\"] \n",
    "with open('D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\RawData\\\\cleanAllCC.txt', 'w+') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                x =(' '.join([word for word in line.split() if word not in stop_words or word !=\"a\" or word !=\"U\"]))\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming for Subtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\output.txt\", \"r\",encoding=\"utf8\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        if word !=\"Advertisement\":\n",
    "            word.lower()\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\output_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Crawl Data Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(flag):\n",
    "    s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\RawData\\\\Clean-cc-\"+flag+\".txt\", \"r\",encoding=\"utf8\").read()\n",
    "    s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "    words = s.split()\n",
    "    stemmed=[]\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            if word !=\"Advertisement\":\n",
    "                word.lower()\n",
    "                stemmed.append(stemmer.stem(word)) \n",
    "                stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "    with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\stemData\\\\stem-cc-\"+flag+\".txt\",'w') as w:\n",
    "        for i in stemmed:\n",
    "            w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_var=[\"G\",\"AZ\",\"MS\",\"FB\",\"U\"]\n",
    "#flag_var=[\"cc\"]\n",
    "for i in flag_var:\n",
    "     stemming(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\RawData\\\\Clean-cc-all.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        if word !=\"Advertisement\":\n",
    "            word.lower()\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\stemData\\\\Stem-cc-all.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwG.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        if word !=\"Advertisement\":\n",
    "            word.lower()\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTwG_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwaz.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        if word !=\"Advertisement\":\n",
    "            word.lower()\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTwaz_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwMS.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        if word !=\"Advertisement\":\n",
    "            word.lower()\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTwMS_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwFB.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        if word !=\"Advertisement\":\n",
    "            word.lower()\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTwFB_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\CleanTwU.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        word.lower()\n",
    "        if word !=\"Advertisement\":\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTwU_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYTimes Data Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYG.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        word.lower()\n",
    "        if word !=\"advertisement\":\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\cleanNYG_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYAZ.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        word.lower()\n",
    "        if word !=\"advertisement\":\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\cleanNYAZ_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYMS.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        word.lower()\n",
    "        if word !=\"advertisement\":\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\cleanNYMS_lemma.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYFB.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        word.lower()\n",
    "        if word !=\"advertisement\":\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\cleanNYFB_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\CleanNYU.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        if word !=\"Advertisement\":\n",
    "            word.lower()\n",
    "            stemmed.append(stemmer.stem(word)) \n",
    "            stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleandata\\\\cleanNYU_stem.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "stemmer =SnowballStemmer(\"english\")\n",
    "stem_words_list=[]\n",
    "#rake_object = rake.Rake(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\cleanTwG.txt\", 5, 3, 4)\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanALLTw.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stemmed.append(stemmer.stem(word))\n",
    "        stemmed.append(\" \")\n",
    "print(type(stemmed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanALLTw_stem.txt\",'w') as w:\n",
    "    for j in stemmed:\n",
    "        w.write((j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanALLTw.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        word.lower()\n",
    "        stemmed.append(stemmer.stem(word)) \n",
    "        stemmed.append(\" \")\n",
    "#stemmed = [stemmer.stem(word) for word in words]\n",
    "#stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanALLTw_lemma.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stem_words_list=[]\n",
    "#rake_object = rake.Rake(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\cleanTw\\\\cleanTwG.txt\", 5, 3, 4)\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleanALLNY.txt\", \"r\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stemmed.append(stemmer.stem(word))\n",
    "        stemmed.append(\" \")\n",
    "\n",
    "print(type(stemmed))\n",
    "    #    except Exception:\n",
    "     #       print(\"bad write\")\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleanALLNY_lemma.txt\",'w') as w:\n",
    "    for i in stemmed:\n",
    "        w.write((i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "s = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleanALLNY.txt\").read()\n",
    "s = re.sub('[^ a-zA-Z*.]', ' ', s) # Remove special characters that might cause problems with stemming\n",
    "words = s.split()\n",
    "stemmed=[]\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stemmed.append(stemmer.stem(word))\n",
    "        stemmed.append(\" \")\n",
    "        #stemmed = ' '.join(stemmed)\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleanALLNY_stem.csv\",'w') as w:\n",
    "    for j in stemmed:\n",
    "        w.write((j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleanALLNY_stem.txt\").read()\n",
    "sss = ss.split()\n",
    "nyss = []\n",
    "for w in sss:\n",
    "    if not w in stop_words:\n",
    "        nyss.append(w)\n",
    "        nyss.append(\" \")\n",
    "with open(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleanALLNY_stem_clean.txt\",'w') as w:\n",
    "    for j in nyss:\n",
    "        w.write((j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleanALLNY_stem_clean.txt\",sep=\"\\t\",header=None)\n",
    "df.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\cleanALLNY_stem_clean.csv\",sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_twfin= [\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\count\\\\finalwc1.txt\",\n",
    "                  \"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\count\\\\finalwc2.txt\",\n",
    "                  \"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\count\\\\finalwc3.txt\"]\n",
    "with open('D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\count\\\\fianltw_wc.txt', 'w+') as outfile:\n",
    "    for fname in filenames_twfin:\n",
    "        xx= []\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                for word in line:\n",
    "                    xx.append(word)\n",
    "                    xx.append(\"\\n\")\n",
    "                #xx.append(' '.join([word for word in line.split() if word not in stop_words ]))\n",
    "            for i in xx:\n",
    "                outfile.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pathtw=\"D:\\\\sem2\\\\dic\\\\Lab\\\\twitterData\\\\cleansubtopics\"\n",
    "df_tw_final = pd.DataFrame()\n",
    "for filename in glob.glob(os.path.join(pathtw, '*.txt')):\n",
    "    f1 = pd.read_csv(filename,header=\"None\")\n",
    "    print(\"zdf\")\n",
    "    pd.concat([df_tw_final,f1],axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Processing of WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mrgoog\\\\c-g1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mrgoog\\\\c-g2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mrgoog\\\\c-g3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"x\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltwg.csv\",sep=\",\",  index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltwg.csv\",sep=\",\",  header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltwg_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "twfb1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mrfb\\\\c-fb1.txt\",sep=\"\\t\", header=None)\n",
    "twfb2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mrfb\\\\c-fb2.txt\",sep=\"\\t\",header=None)\n",
    "twfb3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mrfb\\\\c-fb3.txt\",sep=\"\\t\", header=None)\n",
    "tw_final=pd.concat([twfb1,twfb2,twfb3],axis=0)\n",
    "print(\"x\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_fb.csv\",sep=\",\",  index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_fb.csv\",sep=\",\",  header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltwfb_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mrms\\\\c-ms1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mrms\\\\c-ms2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mrms\\\\c-ms3.txt\",sep=\"\\t\", header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"x\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_ms.csv\",sep=\",\",  index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_ms.csv\",sep=\",\",  header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_ms_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mruber\\\\u1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mruber\\\\u2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mruber\\\\u3.txt\",sep=\"\\t\", header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"x\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_uber.csv\",sep=\",\",  index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_uber.csv\",sep=\",\",  header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_uber_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\count\\\\az1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mraz\\\\az2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\mraz\\\\az3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"x\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_az.csv\", sep=\",\" ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_az.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\finalltw_az_10.csv\", sep=\",\" ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\count\\\\final1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\count\\\\final2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\count\\\\final3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"x\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\count\\\\finalltw.csv\", sep=\",\" ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\count\\\\finalltw.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_count\\\\count\\\\finalltw10.csv\", sep=\",\" ,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Twitter co occurance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxTwitter-Occurance-------Amazon\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\az\\\\az1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\az\\\\az2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\az\\\\az3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxTwitter-Occurance-------Amazon\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\az\\\\az_test.csv\",sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\az\\\\az_test.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\az\\\\az_10.csv\", sep=\",\" ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxTwitter-Occurance-------Facebook\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\FB\\\\fb1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\FB\\\\fb2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\FB\\\\fb3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxTwitter-Occurance-------Facebook\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\FB\\\\fb_occ.csv\",sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\FB\\\\fb_occ.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\FB\\\\fb_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxTwitter-Occurance-------Google\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\google\\\\g1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\google\\\\g2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\google\\\\g3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxTwitter-Occurance-------Google\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\google\\\\g_occ.csv\",sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\google\\\\g_occ.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\google\\\\g_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxTwitter-Occurance-------Microsoft\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\ms\\\\ms1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\ms\\\\ms2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\ms\\\\ms3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxTwitter-Occurance-------Microsoft\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\ms\\\\ms_all.csv\",sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tw_final.sort_values( by=1,ascending=False)\n",
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\ms\\\\ms_all.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\ms\\\\ms_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxTwitter-Occurance----------UBER\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\uber\\\\ub1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\uber\\\\ub2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\uber\\\\ub3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxTwitter-Occurance----------UBER\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\uber\\\\ub_test.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\uber\\\\ub_test.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\uber\\\\ub_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxTwitter-Occurance----------ALL\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\all1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\all2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\all3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxTwitter-Occurance----------ALL\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\all.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\all.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\twitterData\\\\MR_occ\\\\all_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYTimes Word Count Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ny\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\gcn1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\gcn2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\gcn3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"ny\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ny\n"
     ]
    }
   ],
   "source": [
    "ny1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyfb_count\\\\fb1.txt\",sep=\"\\t\", header=None)\n",
    "ny2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyfb_count\\\\fb2.txt\",sep=\"\\t\",header=None)\n",
    "ny3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyfb_count\\\\fb3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([ny1,ny2,ny3],axis=0)\n",
    "print(\"ny\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyfb_count\\\\finallny_fb.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyfb_count\\\\finallny_fb_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyfb_count\\\\finallny_fb.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyfb_count\\\\finallny_fb_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ny\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyaz_count\\\\azlm1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyaz_count\\\\azlm2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyaz_count\\\\azlm3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"ny\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyaz_count\\\\finallny_az_lm1.csv\",sep=\",\",  index=False)\n",
    "#c10=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyaz_count\\\\finallny_az_lm1.csv\",sep=\"\\t\", header=None)\n",
    "#c10.sort_values(by='1')\n",
    "#final10=sort_final[:10]\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyaz_count\\\\finallny_az_lm1_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyaz_count\\\\finallny_az_lm1.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyaz_count\\\\finallny_az_lm1_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ny\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyms_count\\\\ms1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyms_count\\\\ms2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyms_count\\\\ms3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"ny\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyms_count\\\\finallny_ms.csv\",sep=\",\" , index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyms_count\\\\finallny_ms_10.csv\",sep=\",\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyms_count\\\\finallny_ms.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyms_count\\\\finallny_ms_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ny\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyub_count\\\\ub1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyub_count\\\\ub2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyub_count\\\\ub3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"ny\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyub_count\\\\finallny_ub.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY-ALL\n"
     ]
    }
   ],
   "source": [
    "nyaz1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mrcount_stem\\\\ny-final1.txt\",sep=\"\\t\", header=None)\n",
    "nyaz2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mrcount_stem\\\\ny-final2.txt\",sep=\"\\t\",header=None)\n",
    "nyaz3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mrcount_stem\\\\ny-final3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([nyaz1,nyaz2,nyaz3],axis=0)\n",
    "print(\"NY-ALL\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mrcount_stem\\\\ny_all.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mrcount_stem\\\\ny_all.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mrcount_stem\\\\ny_all_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyms_count\\\\finallny_ms.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyms_count\\\\finallny_ms_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyub_count\\\\finallny_ub.csv\", sep=\",\" ,header=None)\n",
    "final10=final[:11]\n",
    "final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyub_count\\\\finallny_ub_10.csv\",sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mr\\\\nyub_count\\\\finallny_ub.csv\")\n",
    "fg=fg[:9]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\\\\\nytimes\\\\mr\\\\fianlny_ub_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mr\\\\nyms_count\\\\finallny_ms.csv\")\n",
    "fg=fg[:9]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\\\\\nytimes\\\\mr\\\\fianlny_ms_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mr\\\\nyg_count\\\\finallny_g.csv\")\n",
    "fg=fg[:9]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\\\\\nytimes\\\\mr\\\\fianlny_g_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\mr\\\\nyaz_count\\\\finallny_az.csv\")\n",
    "fg=fg[:9]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\\\\\nytimes\\\\mr\\\\fianlny_az_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYTimes Co-Occurance Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ny\n"
     ]
    }
   ],
   "source": [
    "nyaz1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\az\\\\az1.txt\",sep=\"\\t\", header=None)\n",
    "nyaz2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\az\\\\az2.txt\",sep=\"\\t\",header=None)\n",
    "nyaz3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\az\\\\az3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([nyaz1,nyaz2,nyaz3],axis=0)\n",
    "print(\"ny\")\n",
    "tw_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_az.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_az.csv\")\n",
    "fg=fg[:9]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_az_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxNY-Cooccur-FB\n"
     ]
    }
   ],
   "source": [
    "nyaz1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\fb\\\\fb1.txt\",sep=\"\\t\", header=None)\n",
    "nyaz2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\fb\\\\fb2.txt\",sep=\"\\t\",header=None)\n",
    "nyaz3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\fb\\\\fb3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([nyaz1,nyaz2,nyaz3],axis=0)\n",
    "print(\"xxxxxxxxxxxNY-Cooccur-FB\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_fb.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_fb.csv\")\n",
    "fg=fg[:11]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_fb_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxNY-Cooccur-Google\n"
     ]
    }
   ],
   "source": [
    "nyaz1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\google\\\\g1.txt\",sep=\"\\t\", header=None)\n",
    "nyaz2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\google\\\\g2.txt\",sep=\"\\t\",header=None)\n",
    "nyaz3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\google\\\\g3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([nyaz1,nyaz2,nyaz3],axis=0)\n",
    "print(\"xxxxxxxxxxxNY-Cooccur-Google\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_g.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_g.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_g_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxNY-Cooccur-Microsoft\n"
     ]
    }
   ],
   "source": [
    "nyaz1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\ms\\\\ms1.txt\",sep=\"\\t\", header=None)\n",
    "nyaz2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\ms\\\\ms2.txt\",sep=\"\\t\",header=None)\n",
    "nyaz3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\ms\\\\ms3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([nyaz1,nyaz2,nyaz3],axis=0)\n",
    "print(\"xxxxxxxxxxxNY-Cooccur-Microsoft\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_ms.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_ms.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_ms_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxNY-Cooccur-Uber\n"
     ]
    }
   ],
   "source": [
    "nyaz1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\uber\\\\ub1.txt\",sep=\"\\t\", header=None)\n",
    "nyaz2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\uber\\\\ub2.txt\",sep=\"\\t\",header=None)\n",
    "nyaz3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\uber\\\\ub3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([nyaz1,nyaz2,nyaz3],axis=0)\n",
    "print(\"xxxxxxxxxxxNY-Cooccur-Uber\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_ub.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_ub.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_ub_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxNY-Cooccur-All\n"
     ]
    }
   ],
   "source": [
    "nyaz1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\all\\\\all1.txt\",sep=\"\\t\", header=None)\n",
    "nyaz2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\all\\\\all2.txt\",sep=\"\\t\",header=None)\n",
    "nyaz3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\all\\\\all3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([nyaz1,nyaz2,nyaz3],axis=0)\n",
    "print(\"xxxxxxxxxxxNY-Cooccur-All\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_all.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_all.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_coocc\\\\finallny_all_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Crawl Word Count Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----Amazon\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\az\\\\az1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\az\\\\az2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\az\\\\az3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----Amazon\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\az\\\\az_sort.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\az\\\\az_sort.csv\")\n",
    "fg=fg[:11]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\az\\\\finall_az_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----Facebook\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\fb\\\\fb1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\fb\\\\fb2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\fb\\\\fb3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----Facebook\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\fb\\\\fb_sort.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\fb\\\\fb_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\fb\\\\finall_fb_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----Google\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\google\\\\g1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\google\\\\g2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\google\\\\g3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----Google\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\google\\\\g_sort.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\google\\\\g_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\google\\\\finall_gb_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----Microsoft\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ms\\\\ms1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ms\\\\ms2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ms\\\\ms3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----Microsoft\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ms\\\\ms_sort.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ms\\\\ms_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ms\\\\finall_ms_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----Uber\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ub\\\\ub1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ub\\\\ub2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ub\\\\ub3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----Uber\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ub\\\\ub_sort.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ub\\\\ub_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\ub\\\\finall_ub_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----ALL\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\all\\\\all1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\all\\\\all2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\all\\\\all3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----ALL\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\all\\\\all_sort.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\all\\\\all_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_count\\\\all\\\\finall_ub_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Crawl Co-Occurance processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----Amazon----co\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\az\\\\az1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\az\\\\az2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\az\\\\az3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----Amazon----co\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\az\\\\az_sort.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\az\\\\az_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\az\\\\finall_az_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----FB----co\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\fb\\\\fb1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\fb\\\\fb2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\fb\\\\fb3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----FB----co\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\fb\\\\fb_sort.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\fb\\\\fb_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\fb\\\\finall_fb_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----google----co\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\google\\\\g1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\google\\\\g2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\google\\\\g3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----google----co\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\google\\\\g_sort.csv\",sep=\",\",  index=False)\n",
    "#final10.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\nytimes\\\\MR_count\\\\nyg_count\\\\finallny_g_10.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\google\\\\g_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\google\\\\finall_g_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----Microsoft----co\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ms\\\\ms1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ms\\\\ms2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ms\\\\ms3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----Microsoft----co\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ms\\\\ms_sort.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ms\\\\ms_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ms\\\\finall_ms_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----Uber----co\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ub\\\\ub1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ub\\\\ub2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ub\\\\ub3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----Uber----co\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ub\\\\ub_sort.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ub\\\\ub_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\ub\\\\finall_ub_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxCC----All----co\n"
     ]
    }
   ],
   "source": [
    "twg1=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\all\\\\all1.txt\",sep=\"\\t\", header=None)\n",
    "twg2=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\all\\\\all2.txt\",sep=\"\\t\",header=None)\n",
    "twg3=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\all\\\\all3.txt\", sep=\"\\t\",header=None)\n",
    "tw_final=pd.concat([twg1,twg2,twg3],axis=0)\n",
    "print(\"xxxxxxxxxCC----All----co\")\n",
    "sort_final=tw_final.sort_values( by=1,ascending=False)\n",
    "#final10=sort_final[:10]\n",
    "sort_final.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\all\\\\all_sort.csv\",sep=\",\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg=pd.read_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\all\\\\all_sort.csv\")\n",
    "fg=fg[:10]\n",
    "fg.to_csv(\"D:\\\\sem2\\\\dic\\\\Lab2\\\\cc\\\\MR_cooccur\\\\all\\\\all_sort_10.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
